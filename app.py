# æ•°æ®æºï¼šOpenAlexï¼ˆå…è´¹å¼€æ”¾è·å–ï¼‰

import streamlit as st
from datetime import date
import requests
import os
from openai import OpenAI
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.figure
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Configure matplotlib to support Chinese characters
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # Fix minus sign display issue


def parse_comma_separated(text: str) -> list[str]:
    """
    Parse comma-separated string supporting both "," and "ï¼Œ" delimiters.
    
    Args:
        text: Comma-separated string
        
    Returns:
        List of non-empty trimmed strings
    """
    # Replace Chinese comma with regular comma for uniform processing
    normalized = text.replace("ï¼Œ", ",")
    # Split by comma, strip whitespace, and filter empty strings
    return [item.strip() for item in normalized.split(",") if item.strip()]


def validate_date_range(start_date: date, end_date: date) -> bool:
    """
    Validate that start date precedes or equals end date.
    
    Args:
        start_date: Beginning of time range
        end_date: End of time range
        
    Returns:
        True if start_date <= end_date, False otherwise
    """
    return start_date <= end_date


@st.cache_data
def identify_top_journals(domain: str) -> list[str]:
    """
    Invokes LLM to generate top-tier journal names.
    
    Args:
        domain: Research field keyword
        
    Returns:
        List of journal names (e.g., ["Nature", "Science"])
    """
    try:
        # Get LLM configuration from environment variables
        api_key = os.environ.get("LLM_API_KEY", "")
        base_url = os.environ.get("LLM_ENDPOINT", None)
        
        # If no API key is configured, return empty list
        if not api_key:
            st.warning("æœªé…ç½® LLM API å¯†é’¥ï¼Œè·³è¿‡æœŸåˆŠç­›é€‰")
            return []
        
        # Initialize OpenAI client (compatible with Qwen API)
        client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)
        
        # Create LLM prompt
        prompt = f"è¾“å‡º ONLY é€—å·åˆ†éš”çš„è‹±æ–‡æœŸåˆŠåï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚é¢†åŸŸï¼š{domain}"
        
        # Call LLM
        response = client.chat.completions.create(
            model="qwen-plus",  # Default Qwen model
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )
        
        # Extract response text
        llm_output = response.choices[0].message.content.strip()
        
        # Parse comma-separated journal names
        journals = parse_comma_separated(llm_output)
        
        return journals
        
    except Exception as e:
        # Log error and continue without journal filtering
        st.warning(f"LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œç»§ç»­ä½¿ç”¨åŸºç¡€åŠŸèƒ½: {str(e)}")
        return []


def reconstruct_abstract_from_inverted_index(inverted_index: dict) -> str:
    """
    Convert OpenAlex inverted index format to full text.
    
    OpenAlex stores abstracts as inverted indices where each word maps to 
    a list of positions where it appears. This function reconstructs the 
    original text by placing words at their correct positions.
    
    Args:
        inverted_index: Dictionary mapping words to position lists
                       e.g., {"hello": [0], "world": [1]}
        
    Returns:
        Reconstructed full text string, or empty string if index is empty
    """
    if not inverted_index:
        return ""
    
    # Create a list to hold words at their positions
    # First, find the maximum position to determine list size
    max_position = 0
    for positions in inverted_index.values():
        if positions:
            max_position = max(max_position, max(positions))
    
    # Initialize list with empty strings
    words = [""] * (max_position + 1)
    
    # Place each word at its positions
    for word, positions in inverted_index.items():
        for pos in positions:
            words[pos] = word
    
    # Join words with spaces
    return " ".join(words)


@st.cache_data
def extract_keywords_batch(papers: list[dict]) -> list[list[str]]:
    """
    Extracts keywords from paper texts using LLM.
    
    Args:
        papers: List of paper dictionaries with 'title' and 'abstract' fields
        
    Returns:
        List of keyword lists, one per paper
    """
    try:
        # Get LLM configuration from environment variables
        api_key = os.environ.get("LLM_API_KEY", "")
        base_url = os.environ.get("LLM_ENDPOINT", None)
        
        # If no API key is configured, return empty list
        if not api_key:
            st.warning("æœªé…ç½® LLM API å¯†é’¥ï¼Œæ— æ³•æå–å…³é”®è¯")
            return []
        
        # Initialize OpenAI client (compatible with Qwen API)
        client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)
        
        keyword_lists = []
        
        # Process each paper
        for paper in papers:
            # Skip papers lacking title or abstract
            title = paper.get("title", "")
            abstract = paper.get("abstract", "")
            
            if not title or not abstract:
                continue
            
            # Create LLM prompt template
            prompt = f"ä»æ–‡æœ¬ä¸­æå–3-5ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼Œç”¨ä¸­æ–‡é€—å·åˆ†éš”ï¼š\næ ‡é¢˜ï¼š{title}\næ‘˜è¦ï¼š{abstract}"
            
            try:
                # Call LLM
                response = client.chat.completions.create(
                    model="qwen-plus",  # Default Qwen model
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=200
                )
                
                # Extract response text
                llm_output = response.choices[0].message.content.strip()
                
                # Parse LLM output using comma-separated parsing function
                keywords = parse_comma_separated(llm_output)
                
                # Add to keyword lists if we got any keywords
                if keywords:
                    keyword_lists.append(keywords)
                    
            except Exception as e:
                # Log error for this paper and continue with others
                st.warning(f"æå–è®ºæ–‡å…³é”®è¯å¤±è´¥ '{title[:50]}...': {str(e)}")
                continue
        
        return keyword_lists
        
    except Exception as e:
        # Log error and return empty list
        st.warning(f"LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œæ— æ³•æå–å…³é”®è¯: {str(e)}")
        return []


@st.cache_data
def fetch_openalex_data(domain: str, start_year: int, end_year: int) -> list[dict]:
    """
    Queries OpenAlex API for publications.
    
    Args:
        domain: Search keyword
        start_year: Beginning of time range (YYYY)
        end_year: End of time range (YYYY)
        
    Returns:
        List of paper dictionaries with 'title', 'abstract', 'id' fields
    """
    try:
        # Construct API endpoint
        url = "https://api.openalex.org/works"
        
        # Construct query parameters
        params = {
            "search": domain,
            "filter": f"publication_year:{start_year}-{end_year}",
            "per_page": 100
        }
        
        # Make API request
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        
        # Parse JSON response
        data = response.json()
        results = data.get("results", [])
        
        # Handle empty results
        if not results:
            st.warning("æœªæ‰¾åˆ° OpenAlex æ•°æ®ï¼Œè¯·æ£€æŸ¥å…³é”®è¯æˆ–æ—¶é—´èŒƒå›´")
            return []
        
        # Extract relevant fields from results and reconstruct abstracts
        papers = []
        for result in results:
            # Get abstract inverted index
            abstract_inverted_index = result.get("abstract_inverted_index", {})
            
            # Skip papers without abstract
            if not abstract_inverted_index:
                continue
            
            # Reconstruct abstract from inverted index
            abstract = reconstruct_abstract_from_inverted_index(abstract_inverted_index)
            
            # Extract title
            title = result.get("title", "")
            
            # Skip papers without title or abstract
            if not title or not abstract:
                continue
            
            paper = {
                "id": result.get("id", ""),
                "title": title,
                "abstract": abstract,
                "publication_year": result.get("publication_year", 0)
            }
            papers.append(paper)
        
        return papers
        
    except requests.exceptions.RequestException as e:
        st.error(f"æ— æ³•è¿æ¥åˆ° OpenAlex APIï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥: {str(e)}")
        return []
    except Exception as e:
        st.error(f"å¤„ç† OpenAlex æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return []


def build_cooccurrence_matrix(keyword_lists: list[list[str]], max_keywords: int = 50) -> pd.DataFrame:
    """
    Constructs keyword co-occurrence matrix.
    
    Args:
        keyword_lists: Keywords extracted from each paper
        max_keywords: Maximum number of keywords to include (default: 50)
        
    Returns:
        DataFrame with keywords as both index and columns
    """
    # Count keyword frequencies
    keyword_freq = {}
    for keywords in keyword_lists:
        for keyword in keywords:
            keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
    
    # Sort keywords by frequency and take top N
    sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
    top_keywords = [kw for kw, _ in sorted_keywords[:max_keywords]]
    
    # Convert to sorted list for consistent ordering
    unique_keywords = sorted(top_keywords)
    
    # Initialize NÃ—N matrix with zeros
    n = len(unique_keywords)
    matrix = [[0 for _ in range(n)] for _ in range(n)]
    
    # Create keyword to index mapping for efficient lookup
    keyword_to_idx = {keyword: idx for idx, keyword in enumerate(unique_keywords)}
    
    # For each paper's keyword list, increment counts for all keyword pairs
    for keywords in keyword_lists:
        # Get unique keywords in this paper (to avoid counting duplicates)
        # Only keep keywords that are in our top keywords list
        unique_paper_keywords = [kw for kw in set(keywords) if kw in keyword_to_idx]
        
        # For each pair (k1, k2) where k1 â‰  k2
        for i, k1 in enumerate(unique_paper_keywords):
            for k2 in unique_paper_keywords[i+1:]:  # Only iterate over remaining keywords to avoid duplicates
                idx1 = keyword_to_idx[k1]
                idx2 = keyword_to_idx[k2]
                
                # Increment both matrix[k1][k2] and matrix[k2][k1] for symmetry
                matrix[idx1][idx2] += 1
                matrix[idx2][idx1] += 1
    
    # Return symmetric matrix as DataFrame
    return pd.DataFrame(matrix, index=unique_keywords, columns=unique_keywords)


def render_heatmap(matrix: pd.DataFrame) -> matplotlib.figure.Figure:
    """
    Generates heatmap visualization.
    
    Args:
        matrix: Co-occurrence matrix
        
    Returns:
        Matplotlib figure object
    """
    # Set dynamic figure size based on matrix dimensions
    # Limit maximum size to prevent memory issues
    n = len(matrix)
    
    # Calculate appropriate figure size (max 20 inches to prevent huge images)
    base_size = 0.4  # inches per keyword
    figsize = min(20, max(8, n * base_size))
    
    # Adjust font sizes based on matrix size
    if n > 30:
        title_fontsize = 14
        label_fontsize = 10
        annot_fontsize = 6
        show_annot = False  # Don't show numbers if too many keywords
    elif n > 20:
        title_fontsize = 15
        label_fontsize = 11
        annot_fontsize = 7
        show_annot = True
    else:
        title_fontsize = 16
        label_fontsize = 12
        annot_fontsize = 8
        show_annot = True
    
    # Create figure and axis with constrained layout
    fig, ax = plt.subplots(figsize=(figsize, figsize), dpi=100)
    
    # Generate heatmap using seaborn
    sns.heatmap(
        matrix,
        cmap="YlGnBu",      # Yellow-Green-Blue color scheme
        annot=show_annot,    # Enable annotations only for smaller matrices
        fmt='g',             # Format for annotations (general format)
        ax=ax,               # Use the created axis
        cbar_kws={'label': 'å…±ç°æ¬¡æ•°'},
        annot_kws={'fontsize': annot_fontsize} if show_annot else {}
    )
    
    # Set title and labels with dynamic font sizes
    ax.set_title('å…³é”®è¯å…±ç°çƒ­åŠ›å›¾', fontsize=title_fontsize, pad=20)
    ax.set_xlabel('å…³é”®è¯', fontsize=label_fontsize)
    ax.set_ylabel('å…³é”®è¯', fontsize=label_fontsize)
    
    # Rotate labels for better readability
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    
    # Adjust layout to prevent label cutoff
    plt.tight_layout()
    
    return fig


def main():
    st.title("ğŸ”¬ ç ”ç©¶çƒ­ç‚¹åˆ†æå·¥å…·")
    st.write("åˆ†æå­¦æœ¯é¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿å’Œçƒ­ç‚¹è¯é¢˜")
    
    # Add cache clearing mechanism in sidebar
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        
        # Keyword limit slider
        max_keywords = st.slider(
            "æœ€å¤§å…³é”®è¯æ•°é‡",
            min_value=10,
            max_value=50,
            value=30,
            step=5,
            help="é™åˆ¶çƒ­åŠ›å›¾ä¸­æ˜¾ç¤ºçš„å…³é”®è¯æ•°é‡ï¼Œé¿å…å›¾ç‰‡è¿‡å¤§"
        )
        
        st.markdown("---")
        
        if st.button("æ¸…é™¤ç¼“å­˜", help="æ¸…é™¤æ‰€æœ‰ç¼“å­˜æ•°æ®ï¼Œå¼ºåˆ¶é‡æ–°è°ƒç”¨ API"):
            st.cache_data.clear()
            st.success("ç¼“å­˜å·²æ¸…é™¤ï¼")
        
        st.markdown("---")
        st.subheader("ğŸ“– ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        1. è¾“å…¥ç ”ç©¶é¢†åŸŸå…³é”®è¯
        2. é€‰æ‹©æ—¶é—´èŒƒå›´
        3. è°ƒæ•´æœ€å¤§å…³é”®è¯æ•°é‡
        4. ç‚¹å‡»"å¼€å§‹åˆ†æ"æŒ‰é’®
        5. æŸ¥çœ‹å…³é”®è¯å…±ç°çƒ­åŠ›å›¾
        """)
        
        st.markdown("---")
        st.caption("æ•°æ®æ¥æº: OpenAlex (å…è´¹å¼€æ”¾)")
    
    # Create text input field for domain keywords
    domain = st.text_input(
        "ğŸ” ç ”ç©¶é¢†åŸŸå…³é”®è¯",
        placeholder="ä¾‹å¦‚ï¼šé‡å­è®¡ç®—ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ",
        help="è¾“å…¥å…³é”®è¯ä»¥æœç´¢å­¦æœ¯è®ºæ–‡"
    )
    
    # Create date input selectors for start and end dates
    col1, col2 = st.columns(2)
    
    with col1:
        start_date = st.date_input(
            "ğŸ“… èµ·å§‹æ—¥æœŸ",
            value=date(2020, 1, 1),
            help="é€‰æ‹©æ—¶é—´èŒƒå›´çš„å¼€å§‹æ—¥æœŸ"
        )
    
    with col2:
        end_date = st.date_input(
            "ğŸ“… ç»“æŸæ—¥æœŸ",
            value=date.today(),
            help="é€‰æ‹©æ—¶é—´èŒƒå›´çš„ç»“æŸæ—¥æœŸ"
        )
    
    # Create submit button to trigger analysis
    submit_button = st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary")
    
    # Process form submission
    if submit_button:
        # Input validation: reject empty domain keywords
        if not domain or not domain.strip():
            st.error("è¯·è¾“å…¥ç ”ç©¶é¢†åŸŸå…³é”®è¯")
        # Date validation: ensure start_date â‰¤ end_date
        elif not validate_date_range(start_date, end_date):
            st.error("å¼€å§‹æ—¥æœŸå¿…é¡»æ—©äºæˆ–ç­‰äºç»“æŸæ—¥æœŸ")
        else:
            # All validations passed - proceed with analysis
            try:
                # Step 1: Identify top-tier journals (optional, for future filtering)
                with st.spinner("ğŸ” æ­£åœ¨è¯†åˆ«ä¸€åŒºæœŸåˆŠ..."):
                    journals = identify_top_journals(domain)
                    if journals:
                        st.info(f"âœ… å·²è¯†åˆ« {len(journals)} ä¸ªä¸€åŒºæœŸåˆŠ")
                
                # Step 2: Fetch papers from OpenAlex
                with st.spinner("ğŸ“š æ­£åœ¨ä» OpenAlex è·å–è®ºæ–‡æ•°æ®..."):
                    start_year = start_date.year
                    end_year = end_date.year
                    papers = fetch_openalex_data(domain, start_year, end_year)
                
                # Check if we got any papers
                if not papers:
                    # Warning already displayed by fetch_openalex_data
                    st.stop()
                
                st.success(f"âœ… å·²ä» OpenAlex è·å– {len(papers)} ç¯‡è®ºæ–‡")
                
                # Step 3: Extract keywords from papers
                with st.spinner("ğŸ¤– æ­£åœ¨ä½¿ç”¨ LLM æå–å…³é”®è¯..."):
                    keyword_lists = extract_keywords_batch(papers)
                
                # Check if we got any keywords
                if not keyword_lists:
                    st.warning("âš ï¸ æ— æ³•ä»è®ºæ–‡ä¸­æå–å…³é”®è¯")
                    st.stop()
                
                st.success(f"âœ… å·²ä» {len(keyword_lists)} ç¯‡è®ºæ–‡ä¸­æå–å…³é”®è¯")
                
                # Count total unique keywords
                all_keywords = set()
                for keywords in keyword_lists:
                    all_keywords.update(keywords)
                total_keywords = len(all_keywords)
                
                # Show info if keywords will be limited
                if total_keywords > max_keywords:
                    st.info(f"â„¹ï¸ å…±æå– {total_keywords} ä¸ªå…³é”®è¯ï¼Œå°†æ˜¾ç¤ºå‡ºç°é¢‘ç‡æœ€é«˜çš„å‰ {max_keywords} ä¸ªå…³é”®è¯")
                
                # Step 4: Build co-occurrence matrix
                with st.spinner("ğŸ“Š æ­£åœ¨æ„å»ºå…±ç°çŸ©é˜µ..."):
                    matrix = build_cooccurrence_matrix(keyword_lists, max_keywords=max_keywords)
                
                # Check if matrix has data
                if matrix.empty or len(matrix) == 0:
                    st.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„å…±ç°æ•°æ®è¿›è¡Œå¯è§†åŒ–")
                    st.stop()
                
                st.success(f"âœ… å·²æ„å»º {len(matrix)}Ã—{len(matrix)} å…±ç°çŸ©é˜µ")
                
                # Step 5: Render heatmap
                with st.spinner("ğŸ¨ æ­£åœ¨ç”Ÿæˆçƒ­åŠ›å›¾..."):
                    fig = render_heatmap(matrix)
                
                # Display final heatmap
                st.subheader("ğŸ“ˆ å…³é”®è¯å…±ç°çƒ­åŠ›å›¾")
                st.pyplot(fig)
                
                # Display summary statistics
                st.subheader("ğŸ“Š åˆ†ææ‘˜è¦")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("åˆ†æè®ºæ–‡æ•°", len(papers))
                with col2:
                    st.metric("å”¯ä¸€å…³é”®è¯æ•°", len(matrix))
                with col3:
                    total_cooccurrences = int(matrix.sum().sum() / 2)  # Divide by 2 because matrix is symmetric
                    st.metric("æ€»å…±ç°æ¬¡æ•°", total_cooccurrences)
                
            except Exception as e:
                # Catch any unexpected errors and display user-friendly message
                st.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                st.info("ğŸ’¡ åº”ç”¨ç¨‹åºä»åœ¨è¿è¡Œã€‚è¯·é‡è¯•æˆ–è°ƒæ•´æœç´¢å‚æ•°ã€‚")

if __name__ == "__main__":
    main()
