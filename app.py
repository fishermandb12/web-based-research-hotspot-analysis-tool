# æ•°æ®æºï¼šOpenAlexï¼ˆå…è´¹å¼€æ”¾è·å–ï¼‰

import streamlit as st
from datetime import date
import requests
import os
from openai import OpenAI
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.figure
from dotenv import load_dotenv
import json

# Note: No longer using .env file, API key configured in UI

# Application version
APP_VERSION = "3.0"
VERSION_FILE = ".app_version.json"

# Configure matplotlib to support Chinese characters
import matplotlib.font_manager as fm
import sys

def setup_chinese_font():
    """
    Setup Chinese font for matplotlib with multiple fallback options.
    """
    # Try to find available Chinese fonts
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    # List of Chinese fonts to try (in order of preference)
    chinese_fonts = [
        'SimHei',           # é»‘ä½“ (Windows)
        'Microsoft YaHei',  # å¾®è½¯é›…é»‘ (Windows)
        'STHeiti',          # åæ–‡é»‘ä½“ (Mac)
        'Arial Unicode MS', # (Mac)
        'PingFang SC',      # è‹¹æ–¹ (Mac)
        'Heiti SC',         # é»‘ä½“-ç®€ (Mac)
        'WenQuanYi Micro Hei',  # æ–‡æ³‰é©¿å¾®ç±³é»‘ (Linux)
        'WenQuanYi Zen Hei',    # æ–‡æ³‰é©¿æ­£é»‘ (Linux)
        'Noto Sans CJK SC',     # æ€æºé»‘ä½“ (Linux)
        'Droid Sans Fallback',  # Android fallback
    ]
    
    # Find first available Chinese font
    found_font = None
    for font in chinese_fonts:
        if font in available_fonts:
            found_font = font
            break
    
    if found_font:
        plt.rcParams['font.sans-serif'] = [found_font] + chinese_fonts
        plt.rcParams['axes.unicode_minus'] = False
        return found_font
    else:
        # If no Chinese font found, use default and warn user
        plt.rcParams['font.sans-serif'] = chinese_fonts + ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        return None

# Setup Chinese font
detected_font = setup_chinese_font()


def check_version_upgrade() -> tuple[bool, str]:
    """
    Check if the application has been upgraded from a previous version.
    
    Returns:
        Tuple of (is_upgrade, previous_version)
        - is_upgrade: True if this is an upgrade from a previous version
        - previous_version: The previous version string, or empty string if new install
    """
    try:
        if os.path.exists(VERSION_FILE):
            with open(VERSION_FILE, 'r', encoding='utf-8') as f:
                version_data = json.load(f)
                previous_version = version_data.get('version', '')
                
                # Check if version has changed
                if previous_version and previous_version != APP_VERSION:
                    return True, previous_version
                else:
                    return False, previous_version
        else:
            # First time running, no version file exists
            return False, ''
    except Exception:
        # If there's any error reading the version file, treat as new install
        return False, ''


def save_current_version():
    """
    Save the current application version to file.
    """
    try:
        version_data = {
            'version': APP_VERSION,
            'updated_at': date.today().isoformat()
        }
        with open(VERSION_FILE, 'w', encoding='utf-8') as f:
            json.dump(version_data, f, indent=2)
    except Exception:
        # Silently fail if we can't write the version file
        pass


def clear_cache_on_upgrade():
    """
    Clear cache if the application has been upgraded.
    Display migration notice to user.
    
    Returns:
        True if cache was cleared due to upgrade, False otherwise
    """
    is_upgrade, previous_version = check_version_upgrade()
    
    if is_upgrade:
        # Clear all cached data
        st.cache_data.clear()
        
        # Display migration notice
        st.info(f"""
        ğŸ”„ **æ£€æµ‹åˆ°ç‰ˆæœ¬å‡çº§: v{previous_version} â†’ v{APP_VERSION}**
        
        ç¼“å­˜å·²è‡ªåŠ¨æ¸…é™¤ä»¥ç¡®ä¿å…¼å®¹æ€§ã€‚
        
        **v{APP_VERSION} ä¸»è¦å˜åŒ–ï¼š**
        - âš ï¸ API Key ç°åœ¨æ˜¯å¿…éœ€çš„ï¼ˆLLM-Only æ¨¡å¼ï¼‰
        - âœ… æ‰€æœ‰å…³é”®è¯æå–å‡ä½¿ç”¨ LLM
        - âœ… 1åŒºæœŸåˆŠç­›é€‰é»˜è®¤å¯ç”¨
        - âŒ ä¸å†æ”¯æŒåŸºäºè§„åˆ™çš„å…³é”®è¯æå–
        
        è¯·åœ¨å·¦ä¾§è¾¹æ é…ç½® API Key åå¼€å§‹ä½¿ç”¨ã€‚
        """)
        
        # Save the new version
        save_current_version()
        
        return True
    else:
        # Not an upgrade, just save current version if not already saved
        if not os.path.exists(VERSION_FILE):
            save_current_version()
        
        return False


def test_openalex_connection() -> bool:
    """
    Test connection to OpenAlex API.
    
    Returns:
        True if connection successful, False otherwise
    """
    try:
        response = requests.get("https://api.openalex.org/", timeout=10)
        return response.status_code == 200
    except:
        return False


def parse_comma_separated(text: str) -> list[str]:
    """
    Parse comma-separated string supporting both "," and "ï¼Œ" delimiters.
    
    Args:
        text: Comma-separated string
        
    Returns:
        List of non-empty trimmed strings
    """
    # Replace Chinese comma with regular comma for uniform processing
    normalized = text.replace("ï¼Œ", ",")
    # Split by comma, strip whitespace, and filter empty strings
    return [item.strip() for item in normalized.split(",") if item.strip()]


def validate_api_key(api_key: str) -> bool:
    """
    Validates that API key is present and non-empty.
    
    Args:
        api_key: User-provided API key
        
    Returns:
        True if valid, False otherwise
    """
    # Check if API key is non-empty and not just whitespace
    if not api_key or not api_key.strip():
        return False
    return True


def validate_date_range(start_date: date, end_date: date) -> bool:
    """
    Validate that start date precedes or equals end date.
    
    Args:
        start_date: Beginning of time range
        end_date: End of time range
        
    Returns:
        True if start_date <= end_date, False otherwise
    """
    return start_date <= end_date


def identify_top_journals(domain: str, api_key: str, endpoint: str) -> list[str]:
    """
    Invokes LLM to generate top-tier journal names (Q1 journals).
    
    Args:
        domain: Research field keyword
        api_key: LLM API key
        endpoint: LLM API endpoint
        
    Returns:
        List of journal names (e.g., ["Nature", "Science"])
    """
    try:
        # If no API key is provided, return empty list
        if not api_key:
            st.warning("âš ï¸ æœªé…ç½® LLM API å¯†é’¥ï¼Œè·³è¿‡æœŸåˆŠç­›é€‰")
            return []
        
        # Initialize OpenAI client (compatible with Qwen API)
        client = OpenAI(api_key=api_key, base_url=endpoint)
        
        # Create LLM prompt for Q1 journals
        prompt = f"""è¯·åˆ—å‡º"{domain}"é¢†åŸŸçš„ä¸­ç§‘é™¢1åŒºæˆ–SCI Q1æœŸåˆŠã€‚

è¦æ±‚ï¼š
1. åªè¾“å‡ºè‹±æ–‡æœŸåˆŠåç§°
2. ç”¨é€—å·åˆ†éš”
3. ä¸è¦å…¶ä»–è§£é‡Šæ–‡å­—
4. åˆ—å‡º5-10ä¸ªé¡¶çº§æœŸåˆŠ

ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼šNature, Science, Cell, Nature Communications, Advanced Materials"""
        
        # Call LLM
        response = client.chat.completions.create(
            model="qwen-plus",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=300,
            timeout=15
        )
        
        # Extract response text
        llm_output = response.choices[0].message.content.strip()
        
        # Parse comma-separated journal names
        journals = parse_comma_separated(llm_output)
        
        return journals
        
    except Exception as e:
        # Log error and continue without journal filtering
        st.warning(f"âš ï¸ æœŸåˆŠè¯†åˆ«å¤±è´¥: {str(e)}")
        return []


def reconstruct_abstract_from_inverted_index(inverted_index: dict) -> str:
    """
    Convert OpenAlex inverted index format to full text.
    
    OpenAlex stores abstracts as inverted indices where each word maps to 
    a list of positions where it appears. This function reconstructs the 
    original text by placing words at their correct positions.
    
    Args:
        inverted_index: Dictionary mapping words to position lists
                       e.g., {"hello": [0], "world": [1]}
        
    Returns:
        Reconstructed full text string, or empty string if index is empty
    """
    if not inverted_index:
        return ""
    
    # Create a list to hold words at their positions
    # First, find the maximum position to determine list size
    max_position = 0
    for positions in inverted_index.values():
        if positions:
            max_position = max(max_position, max(positions))
    
    # Initialize list with empty strings
    words = [""] * (max_position + 1)
    
    # Place each word at its positions
    for word, positions in inverted_index.items():
        for pos in positions:
            words[pos] = word
    
    # Join words with spaces
    return " ".join(words)


def extract_keywords_with_llm_single(papers: list[dict], api_key: str, endpoint: str) -> list[list[str]]:
    """
    Extract keywords using LLM exclusively (no fallback).
    Processes papers one by one for better reliability.
    
    Args:
        papers: List of paper dictionaries
        api_key: LLM API key (required)
        endpoint: LLM API endpoint
        
    Returns:
        List of keyword lists, one per paper
        
    Raises:
        Exception if LLM extraction fails for all papers
    """
    # Initialize OpenAI client
    client = OpenAI(api_key=api_key, base_url=endpoint)
    
    all_keywords = []
    failed_count = 0
    success_count = 0
    failed_papers = []  # Track failed papers for reporting
    
    # Process papers one by one for better reliability
    for i, paper in enumerate(papers, 1):
        try:
            title = paper.get("title", "")
            abstract = paper.get("abstract", "")[:800]  # Limit abstract length
            
            # Skip if no content
            if not title:
                failed_count += 1
                failed_papers.append((i, "æ— æ ‡é¢˜"))
                continue
            
            # Create LLM prompt
            prompt = f"""ä»ä»¥ä¸‹è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦ä¸­æå–3-5ä¸ªæ ¸å¿ƒå…³é”®è¯ã€‚

è¦æ±‚ï¼š
1. æå–å…·ä½“çš„æŠ€æœ¯ã€æ–¹æ³•ã€æ¨¡å‹åç§°ï¼ˆå¦‚"Transformer Architecture"ã€"Quantum Error Correction"ï¼‰
2. é¿å…å®½æ³›æ¦‚å¿µï¼ˆå¦‚"Machine Learning"ã€"Computer Science"ï¼‰
3. ä¼˜å…ˆæå–å¤šè¯ä¸“ä¸šæœ¯è¯­ï¼ˆ2-4ä¸ªè¯ï¼‰
4. ä¸è¦è¾“å‡ºå®Œæ•´çš„è®ºæ–‡æ ‡é¢˜
5. åªè¾“å‡ºå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”

æ ‡é¢˜: {title}
æ‘˜è¦: {abstract if abstract else "æ— æ‘˜è¦"}

è¾“å‡ºæ ¼å¼ï¼šå…³é”®è¯1, å…³é”®è¯2, å…³é”®è¯3
"""
            
            # Call LLM with timeout
            response = client.chat.completions.create(
                model="qwen-plus",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=200,
                timeout=10  # 10 second timeout per paper
            )
            
            # Parse response
            llm_output = response.choices[0].message.content.strip()
            
            # Split keywords by comma
            keywords = [kw.strip() for kw in llm_output.split(',') if kw.strip()]
            
            # Filter out overly long "keywords" (likely full titles)
            # Keep only phrases with 1-5 words
            keywords = [kw for kw in keywords if 1 <= len(kw.split()) <= 5]
            
            # Filter out generic terms
            generic_terms = {'machine learning', 'deep learning', 'artificial intelligence', 
                            'computer science', 'data science', 'neural network'}
            keywords = [kw for kw in keywords if kw.lower() not in generic_terms]
            
            if keywords:
                all_keywords.append(keywords[:6])  # Limit to 6 per paper
                success_count += 1
            else:
                # No valid keywords extracted
                failed_count += 1
                failed_papers.append((i, "æœªæå–åˆ°æœ‰æ•ˆå…³é”®è¯"))
                
        except Exception as e:
            # Skip this paper if LLM fails, continue with others
            failed_count += 1
            error_reason = str(e)
            # Simplify error message for display
            if "timeout" in error_reason.lower():
                error_reason = "å¤„ç†è¶…æ—¶"
            elif "api" in error_reason.lower() or "auth" in error_reason.lower():
                error_reason = "APIè°ƒç”¨å¤±è´¥"
            else:
                error_reason = "æå–å¤±è´¥"
            
            failed_papers.append((i, error_reason))
            
            # Display warning for failed paper (only show first few to avoid spam)
            if failed_count <= 3:
                st.warning(f"âš ï¸ è®ºæ–‡ {i} {error_reason}ï¼Œå·²è·³è¿‡")
            elif failed_count == 4:
                st.warning(f"âš ï¸ æ›´å¤šè®ºæ–‡å¤„ç†å¤±è´¥ï¼Œå°†ç»§ç»­å¤„ç†å‰©ä½™è®ºæ–‡...")
            
            continue
    
    # If all papers failed, raise exception with clear error message in Chinese
    if not all_keywords:
        error_msg = f"âŒ æ‰€æœ‰è®ºæ–‡çš„å…³é”®è¯æå–å‡å¤±è´¥\n\næˆåŠŸ: {success_count}/{len(papers)}\nå¤±è´¥: {failed_count}/{len(papers)}"
        raise Exception(error_msg)
    
    # If some papers failed but we have results, display summary and continue
    if failed_count > 0:
        st.info(f"â„¹ï¸ å…³é”®è¯æå–å®Œæˆï¼šæˆåŠŸ {success_count}/{len(papers)} ç¯‡ï¼Œè·³è¿‡ {failed_count} ç¯‡å¤±è´¥çš„è®ºæ–‡")
    
    return all_keywords


# Rule-based extraction removed in v3.0 - LLM-only mode


@st.cache_data
def fetch_openalex_by_journals(domain: str, start_year: int, end_year: int, journals: list[str], max_total_papers: int = 100) -> list[dict]:
    """
    Queries OpenAlex API directly for each Q1 journal to get substantial paper volume.
    v3.1 Update: Direct journal search with total paper limit for performance.
    
    Args:
        domain: Search keyword (used to filter papers within each journal)
        start_year: Beginning of time range (YYYY)
        end_year: End of time range (YYYY)
        journals: List of Q1 journal names to query
        max_total_papers: Maximum total papers to fetch (default: 100 for performance)
        
    Returns:
        List of paper dictionaries aggregated from all journals (up to max_total_papers)
    """
    # Calculate papers per journal based on total limit
    papers_per_journal = max(5, max_total_papers // len(journals)) if journals else 10
    all_papers = []
    total_journals = len(journals)
    failed_journals = []
    
    # Create a progress bar
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    st.info(f"ğŸ“Š å°†ä» {total_journals} ä¸ªæœŸåˆŠä¸­è·å–è®ºæ–‡ï¼Œæ¯ä¸ªæœŸåˆŠçº¦ {papers_per_journal} ç¯‡ï¼Œæ€»è®¡ä¸è¶…è¿‡ {max_total_papers} ç¯‡")
    
    for i, journal in enumerate(journals):
        # Check if we've reached the limit
        if len(all_papers) >= max_total_papers:
            st.info(f"âœ… å·²è¾¾åˆ°è®ºæ–‡æ•°é‡ä¸Šé™ ({max_total_papers} ç¯‡)ï¼Œåœæ­¢æŸ¥è¯¢")
            break
        try:
            # Update progress
            progress = (i + 1) / total_journals
            progress_bar.progress(progress)
            status_text.text(f"ğŸ” æ­£åœ¨æŸ¥è¯¢æœŸåˆŠ [{i+1}/{total_journals}]: {journal}")
            
            # Construct API endpoint
            url = "https://api.openalex.org/works"
            
            # Construct query parameters
            # Note: OpenAlex doesn't support direct journal name filtering in filter parameter
            # We'll search with domain keyword and filter results by journal name
            params = {
                "filter": f"publication_year:{start_year}-{end_year}",
                "search": domain,
                "per_page": papers_per_journal * 3,  # Fetch more to account for filtering
                "select": "id,title,publication_year,keywords,concepts,abstract_inverted_index,primary_location"
            }
            
            # Make API request with retry mechanism
            max_retries = 3
            retry_delay = 2
            
            for attempt in range(max_retries):
                try:
                    response = requests.get(url, params=params, timeout=60)
                    response.raise_for_status()
                    break  # Success
                except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, requests.exceptions.RequestException) as e:
                    if attempt < max_retries - 1:
                        import time
                        time.sleep(retry_delay)
                    else:
                        raise
            
            # Parse JSON response
            data = response.json()
            results = data.get("results", [])
            
            # Process results and filter by journal name
            journal_papers = []
            for result in results:
                title = result.get("title", "")
                if not title:
                    continue
                
                # Get journal name
                journal_name = ""
                primary_location = result.get("primary_location", {})
                if primary_location and isinstance(primary_location, dict):
                    source = primary_location.get("source", {})
                    if source and isinstance(source, dict):
                        journal_name = source.get("display_name", "")
                
                # Filter by journal name (flexible matching)
                if journal_name:
                    journal_lower = journal.lower().strip()
                    journal_name_lower = journal_name.lower().strip()
                    
                    # Check if this paper is from the target journal
                    is_match = False
                    
                    # Strategy 1: Exact match
                    if journal_lower == journal_name_lower:
                        is_match = True
                    # Strategy 2: Substring match
                    elif journal_lower in journal_name_lower or journal_name_lower in journal_lower:
                        is_match = True
                    # Strategy 3: Word-level matching
                    else:
                        journal_words = [w for w in journal_lower.split() if len(w) > 3]
                        if journal_words:
                            matches = sum(1 for word in journal_words if word in journal_name_lower)
                            if matches >= len(journal_words) * 0.5:
                                is_match = True
                    
                    if not is_match:
                        continue  # Skip papers not from this journal
                
                # Get abstract
                abstract_inverted_index = result.get("abstract_inverted_index", {})
                abstract = ""
                if abstract_inverted_index:
                    abstract = reconstruct_abstract_from_inverted_index(abstract_inverted_index)
                
                # Get keywords and concepts
                keywords = result.get("keywords", [])
                concepts = result.get("concepts", [])
                
                paper = {
                    "id": result.get("id", ""),
                    "title": title,
                    "abstract": abstract,
                    "keywords": keywords,
                    "concepts": concepts,
                    "publication_year": result.get("publication_year", 0),
                    "journal": journal_name
                }
                journal_papers.append(paper)
                
                # Stop if we have enough papers for this journal
                if len(journal_papers) >= papers_per_journal:
                    break
            
            # Add papers from this journal to the total
            all_papers.extend(journal_papers)
            
            # Display result for this journal
            if journal_papers:
                st.success(f"  âœ… {journal}: è·å– {len(journal_papers)} ç¯‡è®ºæ–‡")
            else:
                st.warning(f"  âš ï¸ {journal}: æœªæ‰¾åˆ°è®ºæ–‡")
                failed_journals.append(journal)
        
        except Exception as e:
            st.error(f"  âŒ {journal}: æŸ¥è¯¢å¤±è´¥ ({str(e)})")
            failed_journals.append(journal)
            continue
    
    # Clear progress indicators
    progress_bar.empty()
    status_text.empty()
    
    # Display summary statistics
    successful_journals = total_journals - len(failed_journals)
    st.info(f"ğŸ“Š æŸ¥è¯¢å®Œæˆï¼šæˆåŠŸ {successful_journals}/{total_journals} ä¸ªæœŸåˆŠï¼Œå…±è·å– {len(all_papers)} ç¯‡è®ºæ–‡")
    
    if failed_journals:
        st.warning(f"âš ï¸ ä»¥ä¸‹æœŸåˆŠæœªèƒ½è·å–è®ºæ–‡ï¼š{', '.join(failed_journals[:5])}" + 
                  (f" ç­‰ {len(failed_journals)} ä¸ªæœŸåˆŠ" if len(failed_journals) > 5 else ""))
    
    return all_papers


@st.cache_data
def fetch_openalex_data(domain: str, start_year: int, end_year: int, journals: list[str] = None, max_papers: int = 100) -> list[dict]:
    """
    Queries OpenAlex API for publications.
    v3.1 Update: If journals provided, use direct journal search; otherwise use traditional search.
    
    Args:
        domain: Search keyword
        start_year: Beginning of time range (YYYY)
        end_year: End of time range (YYYY)
        journals: Optional list of journal names (if provided, use direct journal search)
        max_papers: Maximum total papers to fetch (default: 100)
        
    Returns:
        List of paper dictionaries with metadata
    """
    # v3.1: If journals provided, use direct journal search
    if journals:
        return fetch_openalex_by_journals(domain, start_year, end_year, journals, max_total_papers=max_papers)
    
    # Otherwise, use traditional domain keyword search (fallback mode when Q1 filtering disabled)
    try:
        st.info(f"â„¹ï¸ ä½¿ç”¨ä¼ ç»Ÿæœç´¢æ¨¡å¼ï¼ˆæœªå¯ç”¨1åŒºæœŸåˆŠç­›é€‰ï¼‰ï¼Œå°†è·å–æœ€å¤š {max_papers} ç¯‡è®ºæ–‡")
        # Construct API endpoint
        url = "https://api.openalex.org/works"
        
        # Construct query parameters (limit to max_papers)
        params = {
            "search": domain,
            "filter": f"publication_year:{start_year}-{end_year}",
            "per_page": min(max_papers, 100),  # OpenAlex max is 100 per page
            "select": "id,title,publication_year,keywords,concepts,abstract_inverted_index,primary_location"
        }
        
        # Make API request with retry mechanism
        max_retries = 3
        retry_delay = 2  # seconds
        
        for attempt in range(max_retries):
            try:
                # Display retry progress to user
                if attempt > 0:
                    st.info(f"ğŸ”„ æ­£åœ¨è¿›è¡Œç¬¬ {attempt + 1} æ¬¡å°è¯•...")
                
                response = requests.get(url, params=params, timeout=60)  # 60-second timeout per request
                response.raise_for_status()
                break  # Success, exit retry loop
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    st.warning(f"âš ï¸ è¿æ¥è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯• ({attempt + 1}/{max_retries})...")
                    import time
                    time.sleep(retry_delay)
                else:
                    raise
            except requests.exceptions.ConnectionError:
                if attempt < max_retries - 1:
                    st.warning(f"âš ï¸ è¿æ¥å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt + 1}/{max_retries})...")
                    import time
                    time.sleep(retry_delay)
                else:
                    raise
            except requests.exceptions.RequestException:
                if attempt < max_retries - 1:
                    st.warning(f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt + 1}/{max_retries})...")
                    import time
                    time.sleep(retry_delay)
                else:
                    raise
        
        # Parse JSON response
        data = response.json()
        results = data.get("results", [])
        
        # Handle empty results
        if not results:
            # Don't display detailed message here, will be handled in main()
            return []
        
        # Extract relevant fields from results
        papers = []
        filtered_count = 0
        total_results = len(results)
        
        for result in results:
            # Extract title (required)
            title = result.get("title", "")
            if not title:
                continue
            
            # Get journal name for filtering
            journal_name = ""
            primary_location = result.get("primary_location", {})
            if primary_location and isinstance(primary_location, dict):
                source = primary_location.get("source", {})
                if source and isinstance(source, dict):
                    journal_name = source.get("display_name", "")
            
            # Filter by journals if provided (flexible matching)
            if journals and journal_name:
                # Check if paper is from one of the target journals
                # Use flexible matching for journal names
                is_from_target_journal = False
                matched_journal = None
                
                for journal in journals:
                    journal_lower = journal.lower().strip()
                    journal_name_lower = journal_name.lower().strip()
                    
                    # Strategy 1: Exact match
                    if journal_lower == journal_name_lower:
                        is_from_target_journal = True
                        matched_journal = journal
                        break
                    
                    # Strategy 2: Substring match (either direction)
                    if journal_lower in journal_name_lower or journal_name_lower in journal_lower:
                        is_from_target_journal = True
                        matched_journal = journal
                        break
                    
                    # Strategy 3: Word-level matching (for multi-word journal names)
                    # Match if significant words (>3 chars) from target journal appear in actual journal name
                    journal_words = [w for w in journal_lower.split() if len(w) > 3]
                    if journal_words:
                        # Check if at least 50% of significant words match
                        matches = sum(1 for word in journal_words if word in journal_name_lower)
                        if matches >= len(journal_words) * 0.5:
                            is_from_target_journal = True
                            matched_journal = journal
                            break
                
                if not is_from_target_journal:
                    filtered_count += 1
                    continue  # Skip papers not from target journals
            
            # Get abstract (optional, but preferred)
            abstract_inverted_index = result.get("abstract_inverted_index", {})
            abstract = ""
            if abstract_inverted_index:
                abstract = reconstruct_abstract_from_inverted_index(abstract_inverted_index)
            
            # Get keywords (OpenAlex keywords)
            keywords = result.get("keywords", [])
            
            # Get concepts (OpenAlex topic classification)
            concepts = result.get("concepts", [])
            
            paper = {
                "id": result.get("id", ""),
                "title": title,
                "abstract": abstract,
                "keywords": keywords,
                "concepts": concepts,
                "publication_year": result.get("publication_year", 0),
                "journal": journal_name
            }
            papers.append(paper)
        
        # Display filtering statistics if journals were provided
        if journals:
            if len(papers) > 0:
                # Successfully filtered papers
                filter_rate = (len(papers) / total_results * 100) if total_results > 0 else 0
                st.info(f"ğŸ“Š æœŸåˆŠç­›é€‰ç»Ÿè®¡ï¼šä» {total_results} ç¯‡è®ºæ–‡ä¸­ç­›é€‰å‡º {len(papers)} ç¯‡æ¥è‡ª1åŒºæœŸåˆŠçš„è®ºæ–‡ï¼ˆä¿ç•™ç‡: {filter_rate:.1f}%ï¼Œè¿‡æ»¤äº† {filtered_count} ç¯‡ï¼‰")
            else:
                # No papers matched - will fall back to all papers
                st.warning(f"âš ï¸ åœ¨æŒ‡å®šçš„ {len(journals)} ä¸ª1åŒºæœŸåˆŠä¸­æœªæ‰¾åˆ°è®ºæ–‡ï¼ˆå…±æ£€ç´¢åˆ° {total_results} ç¯‡è®ºæ–‡ï¼‰")
                st.info("ğŸ’¡ å°†è¿”å›ç©ºç»“æœï¼Œä¸»ç¨‹åºä¼šè‡ªåŠ¨é‡è¯•æœç´¢æ‰€æœ‰è®ºæ–‡")
        
        return papers
        
    except requests.exceptions.Timeout:
        st.error("âŒ è¿æ¥ OpenAlex API è¶…æ—¶")
        st.info("""
        **å¯èƒ½çš„åŸå› ï¼š**
        - ç½‘ç»œè¿æ¥è¾ƒæ…¢
        - OpenAlex æœåŠ¡å™¨å“åº”æ…¢
        
        **è§£å†³æ–¹æ¡ˆï¼š**
        - æ£€æŸ¥ç½‘ç»œè¿æ¥
        - ç¨åé‡è¯•
        - å°è¯•ä½¿ç”¨ VPN
        """)
        return []
    except requests.exceptions.ConnectionError:
        st.error("âŒ æ— æ³•è¿æ¥åˆ° OpenAlex API")
        st.info("""
        **å¯èƒ½çš„åŸå› ï¼š**
        - ç½‘ç»œè¿æ¥é—®é¢˜
        - é˜²ç«å¢™é˜»æ­¢
        - DNS è§£æå¤±è´¥
        
        **è§£å†³æ–¹æ¡ˆï¼š**
        - æ£€æŸ¥ç½‘ç»œè¿æ¥
        - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
        - å°è¯•ä½¿ç”¨ VPN
        - æ£€æŸ¥æ˜¯å¦èƒ½è®¿é—® https://api.openalex.org
        """)
        return []
    except requests.exceptions.RequestException as e:
        st.error(f"âŒ OpenAlex API è¯·æ±‚å¤±è´¥: {str(e)}")
        return []
    except Exception as e:
        st.error(f"âŒ å¤„ç† OpenAlex æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return []


def build_cooccurrence_matrix(keyword_lists: list[list[str]], max_keywords: int = 50) -> pd.DataFrame:
    """
    Constructs keyword co-occurrence matrix.
    
    Args:
        keyword_lists: Keywords extracted from each paper
        max_keywords: Maximum number of keywords to include (default: 50)
        
    Returns:
        DataFrame with keywords as both index and columns
    """
    # Count keyword frequencies
    keyword_freq = {}
    for keywords in keyword_lists:
        for keyword in keywords:
            keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
    
    # Sort keywords by frequency and take top N
    sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
    top_keywords = [kw for kw, _ in sorted_keywords[:max_keywords]]
    
    # Convert to sorted list for consistent ordering
    unique_keywords = sorted(top_keywords)
    
    # Initialize NÃ—N matrix with zeros
    n = len(unique_keywords)
    matrix = [[0 for _ in range(n)] for _ in range(n)]
    
    # Create keyword to index mapping for efficient lookup
    keyword_to_idx = {keyword: idx for idx, keyword in enumerate(unique_keywords)}
    
    # For each paper's keyword list, increment counts for all keyword pairs
    for keywords in keyword_lists:
        # Get unique keywords in this paper (to avoid counting duplicates)
        # Only keep keywords that are in our top keywords list
        unique_paper_keywords = [kw for kw in set(keywords) if kw in keyword_to_idx]
        
        # For each pair (k1, k2) where k1 â‰  k2
        for i, k1 in enumerate(unique_paper_keywords):
            for k2 in unique_paper_keywords[i+1:]:  # Only iterate over remaining keywords to avoid duplicates
                idx1 = keyword_to_idx[k1]
                idx2 = keyword_to_idx[k2]
                
                # Increment both matrix[k1][k2] and matrix[k2][k1] for symmetry
                matrix[idx1][idx2] += 1
                matrix[idx2][idx1] += 1
    
    # Return symmetric matrix as DataFrame
    return pd.DataFrame(matrix, index=unique_keywords, columns=unique_keywords)


def render_heatmap(matrix: pd.DataFrame) -> matplotlib.figure.Figure:
    """
    Generates heatmap visualization.
    
    Args:
        matrix: Co-occurrence matrix
        
    Returns:
        Matplotlib figure object
    """
    # Set dynamic figure size based on matrix dimensions
    # Limit maximum size to prevent memory issues
    n = len(matrix)
    
    # Calculate appropriate figure size (max 20 inches to prevent huge images)
    base_size = 0.4  # inches per keyword
    figsize = min(20, max(8, n * base_size))
    
    # Adjust font sizes based on matrix size
    if n > 30:
        title_fontsize = 14
        label_fontsize = 10
        annot_fontsize = 6
        show_annot = False  # Don't show numbers if too many keywords
    elif n > 20:
        title_fontsize = 15
        label_fontsize = 11
        annot_fontsize = 7
        show_annot = True
    else:
        title_fontsize = 16
        label_fontsize = 12
        annot_fontsize = 8
        show_annot = True
    
    # Create figure and axis with constrained layout
    fig, ax = plt.subplots(figsize=(figsize, figsize), dpi=100)
    
    # Generate heatmap using seaborn
    sns.heatmap(
        matrix,
        cmap="YlGnBu",      # Yellow-Green-Blue color scheme
        annot=show_annot,    # Enable annotations only for smaller matrices
        fmt='g',             # Format for annotations (general format)
        ax=ax,               # Use the created axis
        cbar_kws={'label': 'å…±ç°æ¬¡æ•°'},
        annot_kws={'fontsize': annot_fontsize} if show_annot else {}
    )
    
    # Set title and labels with dynamic font sizes
    ax.set_title('å…³é”®è¯å…±ç°çƒ­åŠ›å›¾', fontsize=title_fontsize, pad=20)
    ax.set_xlabel('å…³é”®è¯', fontsize=label_fontsize)
    ax.set_ylabel('å…³é”®è¯', fontsize=label_fontsize)
    
    # Rotate labels for better readability
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    
    # Adjust layout to prevent label cutoff
    plt.tight_layout()
    
    return fig


def main():
    st.title("ğŸ”¬ ç ”ç©¶çƒ­ç‚¹åˆ†æå·¥å…·")
    st.write("åˆ†æå­¦æœ¯é¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿å’Œçƒ­ç‚¹è¯é¢˜")
    
    # Check for version upgrade and clear cache if needed
    clear_cache_on_upgrade()
    
    # Show font detection result
    if detected_font:
        st.success(f"âœ… å·²æ£€æµ‹åˆ°ä¸­æ–‡å­—ä½“: {detected_font}")
    else:
        st.warning("âš ï¸ æœªæ£€æµ‹åˆ°ä¸­æ–‡å­—ä½“ï¼Œçƒ­åŠ›å›¾å¯èƒ½æ˜¾ç¤ºæ–¹æ¡†ã€‚è¯·å‚è€ƒä¾§è¾¹æ çš„å­—ä½“å®‰è£…è¯´æ˜ã€‚")
    
    # Add cache clearing mechanism in sidebar
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        
        # LLM API Configuration
        st.subheader("ğŸ”‘ LLM API é…ç½®")
        st.markdown("**âš ï¸ å¿…é¡»é…ç½® API Key æ‰èƒ½ä½¿ç”¨æœ¬å·¥å…·**")
        
        # API Key input - prominent and required
        api_key_input = st.text_input(
            "API Key (å¿…å¡«)",
            value="",
            type="password",
            help="è¾“å…¥ä½ çš„ Qwen API Keyï¼ˆå¿…å¡«é¡¹ï¼‰",
            placeholder="sk-xxxxxxxxxxxxxxxxxxxxxxxx",
            key="api_key_input"
        )
        
        # Store API key in session state
        if 'api_key' not in st.session_state:
            st.session_state.api_key = ""
        
        # Update session state when API key changes
        if api_key_input:
            st.session_state.api_key = api_key_input
        
        # Endpoint input
        endpoint_input = st.text_input(
            "API Endpoint",
            value="https://dashscope.aliyuncs.com/compatible-mode/v1",
            help="API ç«¯ç‚¹åœ°å€",
            key="endpoint_input"
        )
        
        # Store endpoint in session state
        if 'endpoint' not in st.session_state:
            st.session_state.endpoint = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        
        if endpoint_input:
            st.session_state.endpoint = endpoint_input
        
        # Show configuration guide
        with st.expander("ğŸ“– å¦‚ä½•è·å– API Key"):
            st.markdown("""
            1. è®¿é—® [é˜¿é‡Œäº‘ DashScope](https://dashscope.console.aliyun.com/)
            2. æ³¨å†Œ/ç™»å½•è´¦å·
            3. åˆ›å»º API Key
            4. å¤åˆ¶å¹¶ç²˜è´´åˆ°ä¸Šæ–¹è¾“å…¥æ¡†
            """)
        
        # Validate API key and show appropriate status
        is_api_key_valid = validate_api_key(api_key_input)
        
        if is_api_key_valid:
            st.success("âœ… API Key å·²é…ç½®ï¼Œå¯ä»¥å¼€å§‹åˆ†æ")
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥ API Key æ‰èƒ½å¼€å§‹åˆ†æ")
            st.info("ğŸ‘‰ ç‚¹å‡»ä¸‹æ–¹ã€Œå¦‚ä½•è·å– API Keyã€æŸ¥çœ‹è·å–æ–¹æ³•")
        
        st.markdown("---")
        
        # LLM features toggle
        st.subheader("ğŸ¤– æ™ºèƒ½åŠŸèƒ½")
        
        use_journal_filter = st.checkbox(
            "è¯†åˆ«1åŒºæœŸåˆŠ",
            value=True,
            help="ä½¿ç”¨ LLM è¯†åˆ«é¢†åŸŸå†…çš„1åŒºæœŸåˆŠï¼Œåªä¿ç•™è¿™äº›æœŸåˆŠçš„è®ºæ–‡ï¼Œè¿‡æ»¤å…¶ä»–è®ºæ–‡ï¼ˆéœ€è¦ API Keyï¼‰",
            disabled=not bool(api_key_input)
        )
        
        if not api_key_input:
            st.warning("âš ï¸ éœ€è¦é…ç½® API Key æ‰èƒ½ä½¿ç”¨æ™ºèƒ½åŠŸèƒ½")
        
        st.markdown("---")
        
        # Paper limit slider (v3.1 - Performance optimization)
        max_papers = st.slider(
            "æœ€å¤§è®ºæ–‡æ•°é‡",
            min_value=50,
            max_value=300,
            value=100,
            step=50,
            help="é™åˆ¶åˆ†æçš„è®ºæ–‡æ€»æ•°ï¼Œè¾ƒå°‘çš„è®ºæ–‡å¤„ç†æ›´å¿«ï¼ˆæ¨è100ç¯‡ï¼Œçº¦5-10åˆ†é’Ÿï¼‰"
        )
        
        # Keyword limit slider
        max_keywords = st.slider(
            "æœ€å¤§å…³é”®è¯æ•°é‡",
            min_value=10,
            max_value=30,
            value=20,
            step=5,
            help="é™åˆ¶çƒ­åŠ›å›¾ä¸­æ˜¾ç¤ºçš„å…³é”®è¯æ•°é‡ï¼Œé¿å…å›¾ç‰‡è¿‡å¤§"
        )
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("æ¸…é™¤ç¼“å­˜", help="æ¸…é™¤æ‰€æœ‰ç¼“å­˜æ•°æ®ï¼Œå¼ºåˆ¶é‡æ–°è°ƒç”¨ API"):
                st.cache_data.clear()
                # Also update version file to prevent re-showing upgrade notice
                save_current_version()
                st.success("ç¼“å­˜å·²æ¸…é™¤ï¼")
        
        with col2:
            if st.button("æµ‹è¯•ç½‘ç»œ", help="æµ‹è¯• OpenAlex API è¿æ¥"):
                with st.spinner("æµ‹è¯•ä¸­..."):
                    if test_openalex_connection():
                        st.success("âœ… è¿æ¥æ­£å¸¸")
                    else:
                        st.error("âŒ è¿æ¥å¤±è´¥")
                        st.info("è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä½¿ç”¨ VPN")
        
        st.markdown("---")
        st.subheader("ğŸ“– ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        **v3.1 ä¼˜åŒ–ç‰ˆæœ¬**
        
        **ä½¿ç”¨æ­¥éª¤ï¼š**
        1. âš ï¸ **å¿…é¡»é…ç½® API Key**ï¼ˆæ‰€æœ‰åˆ†æéƒ½éœ€è¦ LLMï¼‰
        2. è°ƒæ•´è®ºæ–‡æ•°é‡ï¼ˆæ¨è100ç¯‡ï¼Œçº¦5-10åˆ†é’Ÿï¼‰
        3. è¾“å…¥ç ”ç©¶é¢†åŸŸå…³é”®è¯
        4. é€‰æ‹©æ—¶é—´èŒƒå›´
        5. ç‚¹å‡»"å¼€å§‹åˆ†æ"
        6. æŸ¥çœ‹çƒ­åŠ›å›¾
        
        **åŠŸèƒ½è¯´æ˜ï¼š**
        - ğŸ”‘ **API Key**ï¼šå¿…éœ€ï¼Œç”¨äº LLM æ™ºèƒ½å…³é”®è¯æå–
        - ğŸ” **è¯†åˆ«1åŒºæœŸåˆŠ**ï¼šé»˜è®¤å¯ç”¨ï¼Œç›´æ¥åœ¨é¡¶çº§æœŸåˆŠä¸­æœç´¢è®ºæ–‡
        - ğŸ“Š **æœ€å¤§è®ºæ–‡æ•°é‡**ï¼šé™åˆ¶æ€»è®ºæ–‡æ•°ï¼Œå‡å°‘å¤„ç†æ—¶é—´
        - ğŸ“ˆ **æœ€å¤§å…³é”®è¯æ•°é‡**ï¼šæ§åˆ¶çƒ­åŠ›å›¾å¤§å°
        
        **æ€§èƒ½ä¼˜åŒ–ï¼ˆv3.1ï¼‰ï¼š** 
        - âš¡ å¯è°ƒèŠ‚è®ºæ–‡æ•°é‡ï¼š50-300ç¯‡ï¼ˆé»˜è®¤100ç¯‡ï¼‰
        - âš¡ 100ç¯‡è®ºæ–‡çº¦éœ€5-10åˆ†é’Ÿå¤„ç†
        - âš¡ ç›´æ¥åœ¨Q1æœŸåˆŠä¸­æœç´¢ï¼Œæ— éœ€è¿‡æ»¤
        - âœ… æ‰€æœ‰å…³é”®è¯æå–å‡ä½¿ç”¨ LLM
        
        **å¤„ç†æ—¶é—´ä¼°ç®—ï¼š**
        - 50ç¯‡ï¼šçº¦3-5åˆ†é’Ÿ
        - 100ç¯‡ï¼šçº¦5-10åˆ†é’Ÿï¼ˆæ¨èï¼‰
        - 200ç¯‡ï¼šçº¦10-20åˆ†é’Ÿ
        - 300ç¯‡ï¼šçº¦15-30åˆ†é’Ÿ
        """)
        
        st.markdown("---")
        st.subheader("ğŸ”¤ ä¸­æ–‡å­—ä½“è¯´æ˜")
        if detected_font:
            st.success(f"å½“å‰ä½¿ç”¨: {detected_font}")
        else:
            st.warning("æœªæ£€æµ‹åˆ°ä¸­æ–‡å­—ä½“")
            with st.expander("ğŸ“¥ å­—ä½“å®‰è£…æŒ‡å—"):
                st.markdown("""
                **Windows ç³»ç»Ÿï¼š**
                - é€šå¸¸å·²é¢„è£…ä¸­æ–‡å­—ä½“
                - å¦‚æ˜¾ç¤ºæ–¹æ¡†ï¼Œè¯·é‡å¯åº”ç”¨
                
                **Linux ç³»ç»Ÿï¼š**
                ```bash
                # Ubuntu/Debian
                sudo apt-get install fonts-wqy-zenhei
                
                # æˆ–å®‰è£…æ€æºé»‘ä½“
                sudo apt-get install fonts-noto-cjk
                ```
                
                **Mac ç³»ç»Ÿï¼š**
                - ç³»ç»Ÿè‡ªå¸¦ä¸­æ–‡å­—ä½“
                - å¦‚æœ‰é—®é¢˜è¯·é‡å¯åº”ç”¨
                
                å®‰è£…åè¯·é‡å¯åº”ç”¨ã€‚
                """)
        
        st.markdown("---")
        st.caption("æ•°æ®æ¥æº: OpenAlex (å…è´¹å¼€æ”¾)")
    
    # Create text input field for domain keywords
    domain = st.text_input(
        "ğŸ” ç ”ç©¶é¢†åŸŸå…³é”®è¯",
        placeholder="ä¾‹å¦‚ï¼šé‡å­è®¡ç®—ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ",
        help="è¾“å…¥å…³é”®è¯ä»¥æœç´¢å­¦æœ¯è®ºæ–‡"
    )
    
    # Create date input selectors for start and end dates
    col1, col2 = st.columns(2)
    
    with col1:
        start_date = st.date_input(
            "ğŸ“… èµ·å§‹æ—¥æœŸ",
            value=date(2020, 1, 1),
            help="é€‰æ‹©æ—¶é—´èŒƒå›´çš„å¼€å§‹æ—¥æœŸ"
        )
    
    with col2:
        end_date = st.date_input(
            "ğŸ“… ç»“æŸæ—¥æœŸ",
            value=date.today(),
            help="é€‰æ‹©æ—¶é—´èŒƒå›´çš„ç»“æŸæ—¥æœŸ"
        )
    
    # Validate API key before allowing analysis
    is_api_key_valid = validate_api_key(api_key_input)
    
    # Show warning if API key is not configured
    if not is_api_key_valid:
        st.warning("âš ï¸ è¯·åœ¨å·¦ä¾§è¾¹æ é…ç½® API Key åæ‰èƒ½å¼€å§‹åˆ†æ")
    
    # Create submit button to trigger analysis (disabled if no valid API key in v3.0)
    submit_button = st.button(
        "ğŸš€ å¼€å§‹åˆ†æ", 
        type="primary",
        disabled=not is_api_key_valid,
        help="éœ€è¦é…ç½® API Key æ‰èƒ½å¼€å§‹åˆ†æ" if not is_api_key_valid else "å¼€å§‹åˆ†æç ”ç©¶çƒ­ç‚¹"
    )
    
    # Process form submission
    if submit_button:
        # API Key validation: ensure API key is configured
        if not validate_api_key(api_key_input):
            st.error("âš ï¸ éœ€è¦é…ç½® API Key æ‰èƒ½ä½¿ç”¨")
            st.info("""
            **å¦‚ä½•è·å– API Keyï¼š**
            
            1. è®¿é—® [é˜¿é‡Œäº‘ DashScope](https://dashscope.console.aliyun.com/)
            2. æ³¨å†Œ/ç™»å½•è´¦å·
            3. åœ¨æ§åˆ¶å°åˆ›å»º API Key
            4. å¤åˆ¶ API Key å¹¶ç²˜è´´åˆ°å·¦ä¾§è¾¹æ çš„è¾“å…¥æ¡†ä¸­
            
            **å¸®åŠ©è¯´æ˜ï¼š**
            - API Key æ˜¯å¿…éœ€çš„ï¼Œç”¨äº LLM æ™ºèƒ½å…³é”®è¯æå–
            - è¯·ç¡®ä¿ API Key æœ‰æ•ˆä¸”æœ‰è¶³å¤Ÿçš„é…é¢
            - å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹é˜¿é‡Œäº‘æ–‡æ¡£æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ
            """)
        # Input validation: reject empty domain keywords
        elif not domain or not domain.strip():
            st.error("è¯·è¾“å…¥ç ”ç©¶é¢†åŸŸå…³é”®è¯")
        # Date validation: ensure start_date â‰¤ end_date
        elif not validate_date_range(start_date, end_date):
            st.error("å¼€å§‹æ—¥æœŸå¿…é¡»æ—©äºæˆ–ç­‰äºç»“æŸæ—¥æœŸ")
        else:
            # All validations passed - proceed with analysis
            try:
                # Step 1: Identify top journals (if enabled)
                journals = []
                if use_journal_filter and api_key_input:
                    with st.spinner("ğŸ” æ­£åœ¨è¯†åˆ«1åŒºæœŸåˆŠ..."):
                        journals = identify_top_journals(domain, api_key_input, endpoint_input)
                    
                    if journals:
                        st.success(f"âœ… å·²è¯†åˆ« {len(journals)} ä¸ª1åŒºæœŸåˆŠ")
                        with st.expander("ğŸ“‹ æŸ¥çœ‹æœŸåˆŠåˆ—è¡¨ï¼ˆåªä¼šä¿ç•™è¿™äº›æœŸåˆŠçš„è®ºæ–‡ï¼‰"):
                            st.markdown("**è¯†åˆ«åˆ°çš„1åŒºæœŸåˆŠï¼š**")
                            for i, journal in enumerate(journals, 1):
                                st.write(f"{i}. {journal}")
                        st.info("ğŸ’¡ æ¥ä¸‹æ¥å°†åªä¿ç•™æ¥è‡ªè¿™äº›1åŒºæœŸåˆŠçš„è®ºæ–‡ï¼Œè¿‡æ»¤å…¶ä»–è®ºæ–‡")
                    else:
                        st.info("â„¹ï¸ æœªè¯†åˆ«åˆ°æœŸåˆŠï¼Œå°†æœç´¢æ‰€æœ‰è®ºæ–‡")
                
                # Step 2: Fetch papers from OpenAlex
                with st.spinner("ğŸ“š æ­£åœ¨ä» OpenAlex è·å–è®ºæ–‡æ•°æ®..."):
                    start_year = start_date.year
                    end_year = end_date.year
                    papers = fetch_openalex_data(domain, start_year, end_year, journals if journals else None, max_papers=max_papers)
                
                # If journal filtering resulted in no papers, try without filtering
                if not papers and journals:
                    st.warning("âš ï¸ åœ¨æŒ‡å®šæœŸåˆŠä¸­æœªæ‰¾åˆ°è®ºæ–‡ï¼Œå°è¯•æœç´¢æ‰€æœ‰è®ºæ–‡...")
                    papers = fetch_openalex_data(domain, start_year, end_year, None, max_papers=max_papers)
                
                # Check if we got any papers
                if not papers:
                    # Display enhanced error message for empty results
                    st.error("âŒ æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")
                    st.info("""
                    **å¯èƒ½çš„åŸå› ï¼š**
                    - ğŸ” å…³é”®è¯è¿‡äºå…·ä½“æˆ–æ‹¼å†™é”™è¯¯
                    - ğŸ“… æ—¶é—´èŒƒå›´å†…æ²¡æœ‰ç›¸å…³è®ºæ–‡
                    - ğŸ“š 1åŒºæœŸåˆŠç­›é€‰è¿‡äºä¸¥æ ¼
                    - ğŸŒ OpenAlex æ•°æ®åº“ä¸­æ²¡æœ‰ç›¸å…³æ•°æ®
                    
                    **å»ºè®®çš„è§£å†³æ–¹æ¡ˆï¼š**
                    
                    1. **è°ƒæ•´æœç´¢å…³é”®è¯**
                       - å°è¯•ä½¿ç”¨æ›´é€šç”¨çš„å…³é”®è¯
                       - ä½¿ç”¨è‹±æ–‡å…³é”®è¯ï¼ˆOpenAlex ä¸»è¦æ”¶å½•è‹±æ–‡æ–‡çŒ®ï¼‰
                       - æ£€æŸ¥å…³é”®è¯æ‹¼å†™æ˜¯å¦æ­£ç¡®
                       - ç¤ºä¾‹ï¼šå°† "quantum error correction" æ”¹ä¸º "quantum computing"
                    
                    2. **æ‰©å¤§æ—¶é—´èŒƒå›´**
                       - å°è¯•æ›´é•¿çš„æ—¶é—´è·¨åº¦ï¼ˆå¦‚æœ€è¿‘5å¹´ï¼‰
                       - ç¡®è®¤èµ·å§‹æ—¥æœŸæ—©äºç»“æŸæ—¥æœŸ
                    
                    3. **ç¦ç”¨1åŒºæœŸåˆŠç­›é€‰**
                       - åœ¨å·¦ä¾§è¾¹æ å–æ¶ˆå‹¾é€‰"è¯†åˆ«1åŒºæœŸåˆŠ"
                       - è¿™å°†æœç´¢æ‰€æœ‰æœŸåˆŠçš„è®ºæ–‡ï¼Œè€Œä¸ä»…é™äºé¡¶çº§æœŸåˆŠ
                    
                    4. **å°è¯•å…¶ä»–é¢†åŸŸ**
                       - æŸäº›æ–°å…´é¢†åŸŸå¯èƒ½æ•°æ®è¾ƒå°‘
                       - å°è¯•ç›¸å…³ä½†æ›´æˆç†Ÿçš„ç ”ç©¶é¢†åŸŸ
                    
                    **æç¤ºï¼š** å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥å…ˆç¦ç”¨1åŒºç­›é€‰ï¼ŒæŸ¥çœ‹æ˜¯å¦èƒ½æ‰¾åˆ°è®ºæ–‡ã€‚
                    """)
                    st.stop()
                
                st.success(f"âœ… å·²ä» OpenAlex è·å– {len(papers)} ç¯‡è®ºæ–‡")
                
                # Step 3: Extract keywords from papers using LLM (mandatory in v3.0)
                with st.spinner("ğŸ¤– LLM æ™ºèƒ½æå–å…³é”®è¯..."):
                    try:
                        keyword_lists = extract_keywords_with_llm_single(
                            papers, 
                            api_key=api_key_input,
                            endpoint=endpoint_input
                        )
                    except Exception as e:
                        # Display clear error message in Chinese
                        error_msg = str(e)
                        st.error(f"âŒ LLM å…³é”®è¯æå–å¤±è´¥")
                        
                        # Show the specific error details
                        if "æ‰€æœ‰è®ºæ–‡" in error_msg:
                            st.error(error_msg)
                        else:
                            st.error(f"é”™è¯¯è¯¦æƒ…: {error_msg}")
                        
                        # Display detailed troubleshooting steps in Chinese
                        st.info("""
                        **å¯èƒ½çš„åŸå› ï¼š**
                        - âŒ API Key æ— æ•ˆã€è¿‡æœŸæˆ–é…é¢ä¸è¶³
                        - ğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜æˆ–é˜²ç«å¢™é˜»æ­¢
                        - ğŸ”— API ç«¯ç‚¹é…ç½®é”™è¯¯
                        - â±ï¸ LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨æˆ–å“åº”è¶…æ—¶
                        - ğŸ“„ è®ºæ–‡å†…å®¹æ ¼å¼é—®é¢˜
                        
                        **è§£å†³æ–¹æ¡ˆï¼ˆè¯·æŒ‰é¡ºåºå°è¯•ï¼‰ï¼š**
                        
                        1. **æ£€æŸ¥ API Key**
                           - åœ¨å·¦ä¾§è¾¹æ ç¡®è®¤ API Key å·²æ­£ç¡®è¾“å…¥
                           - è®¿é—® [é˜¿é‡Œäº‘æ§åˆ¶å°](https://dashscope.console.aliyun.com/) éªŒè¯ API Key çŠ¶æ€
                           - ç¡®è®¤ API Key æœ‰è¶³å¤Ÿçš„é…é¢
                        
                        2. **æ£€æŸ¥ç½‘ç»œè¿æ¥**
                           - ç‚¹å‡»å·¦ä¾§è¾¹æ çš„"æµ‹è¯•ç½‘ç»œ"æŒ‰é’®
                           - ç¡®è®¤å¯ä»¥è®¿é—® OpenAlex API
                           - å¦‚åœ¨å›½å†…ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ VPN
                        
                        3. **æ£€æŸ¥ API ç«¯ç‚¹**
                           - ç¡®è®¤ç«¯ç‚¹åœ°å€ä¸º: `https://dashscope.aliyuncs.com/compatible-mode/v1`
                           - å¦‚ä½¿ç”¨å…¶ä»– LLM æœåŠ¡ï¼Œè¯·ç¡®è®¤ç«¯ç‚¹æ­£ç¡®
                        
                        4. **è°ƒæ•´æœç´¢å‚æ•°**
                           - å°è¯•ç¼©çŸ­æ—¶é—´èŒƒå›´ä»¥å‡å°‘è®ºæ–‡æ•°é‡
                           - å°è¯•æ›´å…·ä½“çš„å…³é”®è¯
                        
                        5. **ç¨åé‡è¯•**
                           - LLM æœåŠ¡å¯èƒ½æš‚æ—¶ç¹å¿™
                           - ç­‰å¾…å‡ åˆ†é’Ÿåé‡æ–°å°è¯•
                        
                        **é‡è¦æç¤ºï¼š** v3.0 ç‰ˆæœ¬ä»…æ”¯æŒ LLM æå–ï¼Œæ— è§„åˆ™æå–å¤‡é€‰æ–¹æ¡ˆã€‚å¿…é¡»è§£å†³ LLM è¿æ¥é—®é¢˜æ‰èƒ½ç»§ç»­ã€‚
                        """)
                        st.stop()
                
                # Check if we got any keywords
                if not keyword_lists:
                    st.warning("âš ï¸ æ— æ³•ä»è®ºæ–‡ä¸­æå–å…³é”®è¯")
                    st.stop()
                
                st.success(f"âœ… å·²ä» {len(keyword_lists)} ç¯‡è®ºæ–‡ä¸­æå–å…³é”®è¯")
                
                # Count total unique keywords
                all_keywords = set()
                for keywords in keyword_lists:
                    all_keywords.update(keywords)
                total_keywords = len(all_keywords)
                
                # Show info if keywords will be limited
                if total_keywords > max_keywords:
                    st.info(f"â„¹ï¸ å…±æå– {total_keywords} ä¸ªå…³é”®è¯ï¼Œå°†æ˜¾ç¤ºå‡ºç°é¢‘ç‡æœ€é«˜çš„å‰ {max_keywords} ä¸ªå…³é”®è¯")
                
                # Step 4: Build co-occurrence matrix
                with st.spinner("ğŸ“Š æ­£åœ¨æ„å»ºå…±ç°çŸ©é˜µ..."):
                    matrix = build_cooccurrence_matrix(keyword_lists, max_keywords=max_keywords)
                
                # Check if matrix has data
                if matrix.empty or len(matrix) == 0:
                    st.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„å…±ç°æ•°æ®è¿›è¡Œå¯è§†åŒ–")
                    st.stop()
                
                st.success(f"âœ… å·²æ„å»º {len(matrix)}Ã—{len(matrix)} å…±ç°çŸ©é˜µ")
                
                # Step 5: Render heatmap
                with st.spinner("ğŸ¨ æ­£åœ¨ç”Ÿæˆçƒ­åŠ›å›¾..."):
                    fig = render_heatmap(matrix)
                
                # Display final heatmap
                st.subheader("ğŸ“ˆ å…³é”®è¯å…±ç°çƒ­åŠ›å›¾")
                st.pyplot(fig)
                
                # Display summary statistics
                st.subheader("ğŸ“Š åˆ†ææ‘˜è¦")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("åˆ†æè®ºæ–‡æ•°", len(papers))
                with col2:
                    st.metric("å”¯ä¸€å…³é”®è¯æ•°", len(matrix))
                with col3:
                    total_cooccurrences = int(matrix.sum().sum() / 2)  # Divide by 2 because matrix is symmetric
                    st.metric("æ€»å…±ç°æ¬¡æ•°", total_cooccurrences)
                
            except Exception as e:
                # Catch any unexpected errors and display user-friendly message
                st.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                st.info("ğŸ’¡ åº”ç”¨ç¨‹åºä»åœ¨è¿è¡Œã€‚è¯·é‡è¯•æˆ–è°ƒæ•´æœç´¢å‚æ•°ã€‚")

if __name__ == "__main__":
    main()
